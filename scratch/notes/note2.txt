vocab
x1, x2, x3...., x5k

batch unique tokens:
x1, x2, ..., x10

original input space/program: 
x1 x2 x3 x4 x5 <-- one-hot representation
each x_i: [1x5000]
each program, x: [5x5000] --> [5x1500] (hidden dim space R^1500) --> decoder --> [5x5000]

step 1.
select random z, optimize u
x* = x1 x2 *x6* x4 x5

step 2.
fix u, optimize z:
for z=1, {x2, ..., x10}\{x1} x2 x6 x4 x5
x2 x2 x6 ..
x3 x2 x6 ..
x4 x2 x6 ..
...
x10 x2 x6 ..

for z=2, x1 {x1, x2, ..., x10}\{x2} x6 x4 x5
for z=3, x1 x2 {x1, x2, ..., x10}\{x6} x4 x5
and so on..

alternate, step2'
for z=1, {x2, .., x10}\{x1} x2 x6 x4 x5
should be approx equivalent
l = z \cdot {avg{R(x2), R(x3), ... R(x10)} R(x2) R(x6) R(x4) R(x5)}
((decoding))



so total number of model evaluations -- 
O(N) (for z) \times average(O(B)) (for # of unique tokens in B)

argmax over it.

======================================
1. gradient wrt x: N x |V|
x_i : 1 x |V| <-- one-hot vector

2. x = x - alpha. gradient

3. u = argmax(x) <-- Yefet et al.
replace x_i :: u
======================================

-- different loss function in our case. 
-- 

label: get method name EOS
prediction: get shash name EOS

def get_annual_tax (input):
  a = input + c

x = [def, :, input, a, =, input, +, c] <-- N = 8
targets = [get, annual, tax] <-- K = 3
model(x) --> y1, y2, y3, ... yk

get set annual half year 

 x = [x'1 x2 x3 .. x5] <-- N
 y = model(x)

 N_i length input -> variable length output (K_i)

 y1 y2  y3 .. y_K


 ===================================

 def X :
   x = y + z
   xx = rr.pp

==========================
Token replacement

def X:
    R1 = R2 + R3
    R4 = R5.R6
==========================
Q^1

variable name:
def X:
    R1 = R2 + R3
    R4 = R5.pp

param name
 def X :
   x = y + z
   xx = rr.R6


cross-entropy
L = cross-ent(pred, y)
X_i <-- one hot encoding
Q = dL/dX_i
argmax(Q) 

our case
L = cross-ent(pred, y)
X_i <-- one hot encoding
Q = dL/dX_i
X_i = X_i + alpha. Q
argmax(X_i) 

















